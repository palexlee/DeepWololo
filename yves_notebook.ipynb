{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python_files/')\n",
    "from model_trainer import ModelTrainer\n",
    "from utils import *\n",
    "from data_loader import load_data\n",
    "from nn_modules import View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.4.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Using MNIST\n",
      "** Reduce the data-set (use --full for the full thing)\n",
      "** Use 1000 train and 1000 test samples\n"
     ]
    }
   ],
   "source": [
    "train_input, train_target, test_input, test_target = load_data(cifar=False, normalize=True, flatten=False, full=False)\n",
    "train_target = train_target.long()\n",
    "test_target = test_target.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fe6f070b7f0>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "i = 420\n",
    "plt.imshow(train_input[i, 0, :, :])\n",
    "print(train_target[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) LeNet with disjoint trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_input, train_target)\n",
    "test_dataset = (test_input, test_target)\n",
    "\n",
    "in_channels = train_input.shape[1]\n",
    "out_channels = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "LeNet_gen = lambda : Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_layers_dims(LeNet_gen(), train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = np.array(train_target.tolist())\n",
    "unique, counts = np.unique(target_classes, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "crossEntropyClassesWeigths = Tensor(counts/len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target)\n",
    "y_hat_fun = lambda y: y.argmax(1)\n",
    "\n",
    "LeNet = LeNet_gen()\n",
    "loss_fun = nn.CrossEntropyLoss(weight=crossEntropyClassesWeigths)\n",
    "learning_rate = 0.05\n",
    "\n",
    "mt = ModelTrainer(LeNet, loss_fun, optim.SGD(LeNet.parameters(), lr=learning_rate), y_hat_fun, crit_fun)\n",
    "hist = mt.fit(train_dataset, test_dataset, epochs=100, batch_size=100, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Testing with a fraction of data from trainset (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biasedset(train_dataset, test_dataset, bias=0.2, plot=False):\n",
    "    train_input = train_dataset[0]\n",
    "    train_target = train_dataset[1]\n",
    "    test_input = test_dataset[0]\n",
    "    test_target = test_dataset[1]\n",
    "    \n",
    "    biased_input = test_input.clone()\n",
    "    biased_target = test_target.clone()\n",
    "\n",
    "    N = len(train_input)\n",
    "    indices_add = np.random.choice(np.arange(0, N), int(bias * N))\n",
    "\n",
    "    M = len(test_input)\n",
    "    indices_replace = np.random.choice(np.arange(0, M), int(bias * M))\n",
    "\n",
    "    for i in range(len(indices_add)):\n",
    "        biased_input[indices_replace[i]] = train_input[indices_add[i]]\n",
    "        biased_target[indices_replace[i]] = train_target[indices_add[i]]\n",
    "        \n",
    "    if plot:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(biased_input[indices_replace[0]][0])\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(test_input[indices_replace[0]][0])\n",
    "\n",
    "    return (biased_input, biased_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_dataset = generate_biasedset(train_dataset, test_dataset, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = np.array(biased_dataset[1].tolist())\n",
    "unique, counts = np.unique(target_classes, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "crossEntropyClassesWeigths = Tensor(counts/len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target)\n",
    "y_hat_fun = lambda y: y.argmax(1)\n",
    "\n",
    "LeNet = LeNet_gen()\n",
    "loss_fun = nn.CrossEntropyLoss(weight=crossEntropyClassesWeigths)\n",
    "learning_rate = 0.1\n",
    "\n",
    "mt = ModelTrainer(LeNet, loss_fun, optim.SGD(LeNet.parameters(), lr=learning_rate), y_hat_fun, crit_fun)\n",
    "hist = mt.fit(train_dataset, biased_dataset, epochs=40, batch_size=100, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biasedset(train_dataset, test_dataset, bias=0.2, plot=False):\n",
    "    train_input = train_dataset[0]\n",
    "    train_target = train_dataset[1]\n",
    "    test_input = test_dataset[0]\n",
    "    test_target = test_dataset[1]\n",
    "    \n",
    "    biased_input = test_input.clone()\n",
    "    biased_target = test_target.clone()\n",
    "\n",
    "    N = len(train_input)\n",
    "    indices_add = np.random.choice(np.arange(0, N), int(bias * N))\n",
    "\n",
    "    M = len(test_input)\n",
    "    indices_replace = np.random.choice(np.arange(0, M), int(bias * M))\n",
    "\n",
    "    for i in range(len(indices_add)):\n",
    "        biased_input[indices_replace[i]] = train_input[indices_add[i]]\n",
    "        biased_target[indices_replace[i]] = train_target[indices_add[i]]\n",
    "        \n",
    "    if plot:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(biased_input[indices_replace[0]][0])\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(test_input[indices_replace[0]][0])\n",
    "\n",
    "    return (biased_input, biased_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_dataset = generate_biasedset(train_dataset, test_dataset, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = np.array(biased_dataset[1].tolist())\n",
    "unique, counts = np.unique(target_classes, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "crossEntropyClassesWeigths = Tensor(counts/len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target)\n",
    "y_hat_fun = lambda y: y.argmax(1)\n",
    "\n",
    "LeNet = LeNet_gen()\n",
    "loss_fun = nn.CrossEntropyLoss(weight=crossEntropyClassesWeigths)\n",
    "learning_rate = 0.1\n",
    "\n",
    "mt = ModelTrainer(LeNet, loss_fun, optim.SGD(LeNet.parameters(), lr=learning_rate), y_hat_fun, crit_fun)\n",
    "hist = mt.fit(train_dataset, biased_dataset, epochs=40, batch_size=100, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Hooks definition to get layers activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_output = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spyOn(model, dict_, name):\n",
    "    def hook_fun (m, input_, output_): \n",
    "        dict_[name] = output_\n",
    "        print(\"captured output at layer : \"+str(m)) \n",
    "            \n",
    "    handle = model.register_forward_hook(hook_fun)\n",
    "    return handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Can use the handle to remove the hook, don't execute twice on \n",
    "#the same handle, or you won't be able to remove the first\n",
    "#instance of the hook\n",
    "handle = spyOn(LeNet.conv2, activation_output, 'convolution 2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet.eval()\n",
    "LeNet(train_input[0].reshape(-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_output['convolution 2'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(alexnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target.reshape(-1, 1))\n",
    "y_hat_fun = lambda y: y.max(1)\n",
    "\n",
    "mt = ModelTrainer(alexnet32, criterion, optimizer, y_hat_fun, crit_fun)\n",
    "hist = mt.fit((train_input, train_target), (test_input, test_target), epochs=500, batch_size=250, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
