{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import sys\n",
    "sys.path.append('./python_files/')\n",
    "from model_trainer import ModelTrainer\n",
    "from utils import *\n",
    "from data_loader import load_data\n",
    "from nn_modules import View\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cross_decomposition import CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, train_target, test_input, test_target = load_data(cifar=False, normalize=True, flatten=False)\n",
    "train_target = train_target.long()\n",
    "test_target = test_target.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 420\n",
    "plt.imshow(train_input[i, 0, :, :])\n",
    "print(train_target[i].item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) LeNet with disjoint trainset and testset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = (train_input, train_target)\n",
    "test_dataset = (test_input, test_target)\n",
    "\n",
    "in_channels = train_input.shape[1]\n",
    "out_channels = 10\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size = 5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size = 5)\n",
    "        self.fc1 = nn.Linear(256, 200)\n",
    "        self.fc2 = nn.Linear(200, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), kernel_size = 3))\n",
    "        x = F.relu(F.max_pool2d(self.conv2(x), kernel_size = 2))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "LeNet_gen = lambda : Net()\n",
    "LeNet_gen2 = lambda : nn.Sequential(\n",
    "        nn.Conv2d(in_channels, 6, kernel_size=(5, 5)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "        nn.Conv2d(6, 16, kernel_size=(5,5)),\n",
    "        nn.ReLU(),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "\n",
    "        View([-1]),\n",
    "\n",
    "        nn.Linear(256, 120),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "\n",
    "        nn.Linear(120, 84),\n",
    "        nn.ReLU(),\n",
    "        nn.Dropout(),\n",
    "\n",
    "        nn.Linear(84, out_channels),\n",
    "        nn.Sigmoid()\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#debug_layers_dims(LeNet_gen(), train_input.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = np.array(train_target.tolist())\n",
    "unique, counts = np.unique(target_classes, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "crossEntropyClassesWeigths = Tensor(counts/len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target)\n",
    "y_hat_fun = lambda y: y.argmax(1)\n",
    "\n",
    "LeNet = LeNet_gen()\n",
    "loss_fun = nn.CrossEntropyLoss(weight=crossEntropyClassesWeigths)\n",
    "learning_rate = 0.1\n",
    "\n",
    "mt = ModelTrainer(LeNet, loss_fun, optim.SGD(LeNet.parameters(), lr=learning_rate), y_hat_fun, crit_fun)\n",
    "hist = mt.fit(train_dataset, test_dataset, epochs=40, batch_size=100, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet.eval()\n",
    "i = 36\n",
    "print(LeNet(test_input[i].reshape(-1, 1, 28, 28)).argmax(1))\n",
    "test_target[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_input[36, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Testing with a fraction of data from trainset (10%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_biasedset(train_dataset, test_dataset, bias=0.2, plot=False):\n",
    "    train_input = train_dataset[0]\n",
    "    train_target = train_dataset[1]\n",
    "    test_input = test_dataset[0]\n",
    "    test_target = test_dataset[1]\n",
    "    \n",
    "    biased_input = test_input.clone()\n",
    "    biased_target = test_target.clone()\n",
    "\n",
    "    N = len(train_input)\n",
    "    indices_add = np.random.choice(np.arange(0, N), int(bias * N))\n",
    "\n",
    "    M = len(test_input)\n",
    "    indices_replace = np.random.choice(np.arange(0, M), int(bias * M))\n",
    "\n",
    "    for i in range(len(indices_add)):\n",
    "        biased_input[indices_replace[i]] = train_input[indices_add[i]]\n",
    "        biased_target[indices_replace[i]] = train_target[indices_add[i]]\n",
    "        \n",
    "    if plot:\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(biased_input[indices_replace[0]][0])\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(test_input[indices_replace[0]][0])\n",
    "\n",
    "    return (biased_input, biased_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "biased_dataset = generate_biasedset(train_dataset, test_dataset, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_classes = np.array(biased_dataset[1].tolist())\n",
    "unique, counts = np.unique(target_classes, return_counts=True)\n",
    "\n",
    "print(dict(zip(unique, counts)))\n",
    "crossEntropyClassesWeigths = Tensor(counts/len(target_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target)\n",
    "y_hat_fun = lambda y: y.argmax(1)\n",
    "\n",
    "LeNet = LeNet_gen()\n",
    "loss_fun = nn.CrossEntropyLoss(weight=crossEntropyClassesWeigths)\n",
    "learning_rate = 0.1\n",
    "\n",
    "mt = ModelTrainer(LeNet, loss_fun, optim.SGD(LeNet.parameters(), lr=learning_rate), y_hat_fun, crit_fun)\n",
    "hist = mt.fit(train_dataset, biased_dataset, epochs=40, batch_size=100, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_output = dict()\n",
    "hook_layer_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hook(m, input_, output):\n",
    "    print(str(m) + ' got ' + str(input_[0].size()))\n",
    "    activation_output['Layer '+str(hook_layer_counter)+' : '+str(m)] = output\n",
    "    hook_layer_counter += 1\n",
    "    \n",
    "def hook_reset(m, input_):\n",
    "    print('\\n===Reset the hook variables===\\n')\n",
    "    activation_output = dict()\n",
    "    hook_layer_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_forwark_hooks(model, hook_fun):\n",
    "    handle_list = list()\n",
    "    \n",
    "    handle_list.append(model.register_forward_pre_hook(hook_reset))\n",
    "    \n",
    "    for m in model.modules():\n",
    "         handle_list.append(m.register_forward_hook(hook_fun))\n",
    "            \n",
    "    return handle_list\n",
    "\n",
    "def remove_hooks(handle_list):\n",
    "    for h in handle_list:\n",
    "         h.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "handles = add_forwark_hooks(LeNet, my_hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_hooks(handles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet.eval()\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LeNet(train_input[10:12].reshape(-1, 1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation_output.get('Sigmoid()').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adamax(alexnet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crit_fun = lambda input_, target : (input_, target.reshape(-1, 1))\n",
    "y_hat_fun = lambda y: y.max(1)\n",
    "\n",
    "mt = ModelTrainer(alexnet32, criterion, optimizer, y_hat_fun, crit_fun)\n",
    "hist = mt.fit((train_input, train_target), (test_input, test_target), epochs=500, batch_size=250, verbose=10)\n",
    "mt.plot_training(\"Learning curves\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
